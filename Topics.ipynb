{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U spacy\n",
    "#!pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
    "\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas version is 1.0.1\n",
      "C:/Users/20193703/Documents/University/Data challenge/CSV/full_basic_with_sentiment.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20193703\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (2,3,5,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0        int64\n",
      "_id              object\n",
      "created_at       object\n",
      "id               object\n",
      "id_str          float64\n",
      "text             object\n",
      "user.id_str     float64\n",
      "lang             object\n",
      "timestamp_ms    float64\n",
      "sentiment         int64\n",
      "dtype: object\n",
      "Please specify your DataFrame name: df_sentiment\n",
      "created_at converted to string, use timestamp for datetime dtype\n",
      "Skip id, use id_str instead\n",
      "timestamp converted to datetime dtype\n",
      "Dtypes after transformation:\n",
      " Unnamed: 0               int64\n",
      "created_at              object\n",
      "id                      object\n",
      "id_str                   Int64\n",
      "text                    object\n",
      "user.id_str              Int64\n",
      "lang                    object\n",
      "timestamp_ms    datetime64[ns]\n",
      "sentiment                Int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "%run import_file.ipynb #import full basic with sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_text = df_sentiment['text']\n",
    "df_text.dropna(inplace = True)\n",
    "df_result= df_text.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate, clean and tokenize the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "def tokenize_trans(text):\n",
    "    '''Tokenize and translate the inserted text''' \n",
    "    tokens_trans = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            tokens_trans.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            tokens_trans.append('SCREEN_NAME')\n",
    "        else:\n",
    "            tokens_trans.append(token.lower_)\n",
    "    return tokens_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\20193703\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_rootword(word):\n",
    "    \"\"\"Changes a  word to its rootword\"\"\"\n",
    "    root = wn.morphy(word)\n",
    "    if root is None:\n",
    "        return word\n",
    "    else:\n",
    "        return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\20193703\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Combines other functions to return the tokens\"\"\"\n",
    "    tokens = tokenize_trans(text) #deletes url, hashtags and @\n",
    "    tokens = [token for token in tokens if len(token) > 4] #checks if word is > 5\n",
    "    tokens = [token for token in tokens if token not in en_stop] #checks if word is not a stopword\n",
    "    tokens = [get_rootword(token) for token in tokens] #swaps the token to its rootword\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = []\n",
    "for line in df_text:\n",
    "    tokens = preprocess_text(line)\n",
    "    #print(tokens)\n",
    "    data_text.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_baggage = ['baggage', 'luggage']\n",
    "topic_delay = ['delay']\n",
    "topic_poluttion = ['pollute']\n",
    "topic_cancel = ['cancel']\n",
    "topic_service = ['service']\n",
    "topic_ticket = ['ticket']\n",
    "topic_corona = ['corona']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62277\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "import time\n",
    "df_result['Topic'] = ''\n",
    "df_result['Baggage'] = 0\n",
    "df_result['Delay'] = 0\n",
    "df_result['Pollution'] = 0\n",
    "df_result['Cancel'] = 0\n",
    "df_result['Service'] = 0\n",
    "df_result['Ticket'] = 0\n",
    "df_result['Corona'] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start = time.time_ns() // 1000000\n",
    "\n",
    "for l in data_text:\n",
    "    s = \"\"\n",
    "    if topic_baggage[0] in l:\n",
    "        s = s +'1'\n",
    "    if topic_baggage[1] in l:\n",
    "        s = s +'1'\n",
    "    if topic_delay[0] in l:\n",
    "        s = s +'2'\n",
    "    if topic_poluttion[0] in l:\n",
    "        s = s +'3'\n",
    "    if topic_cancel[0] in l:\n",
    "        s = s + '4'\n",
    "    if topic_service[0] in l:\n",
    "        s = s +'5'\n",
    "    if topic_ticket[0] in l:\n",
    "        s = s +'6'\n",
    "    if topic_corona[0] in l:\n",
    "        s = s +'7'\n",
    "    df_result['Topic'].iat[index] = s\n",
    "    \n",
    "    \n",
    "    if topic_baggage[0] in l:\n",
    "        df_result['Baggage'].iat[index] = 1\n",
    "    if topic_baggage[1] in l:\n",
    "        df_result['Baggage'].iat[index] = 1\n",
    "    if topic_delay[0] in l:\n",
    "        df_result['Delay'].iat[index] = 1\n",
    "    if topic_poluttion[0] in l:\n",
    "        df_result['Pollution'].iat[index] = 1\n",
    "    if topic_cancel[0] in l:\n",
    "        df_result['Cancel'].iat[index] = 1\n",
    "    if topic_service[0] in l:\n",
    "         df_result['Service'].iat[index] = 1\n",
    "    if topic_ticket[0] in l:\n",
    "        df_result['Ticket'].iat[index] = 1\n",
    "    if topic_corona[0] in l:\n",
    "         df_result['Corona'].iat[index] = 1\n",
    "    index = index + 1\n",
    "end = time.time_ns() // 1000000\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result['id_str'] = df['id_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = {\n",
    "    '1' : 'baggage',\n",
    "    '2' : 'delay',\n",
    "    '3' : 'pollute',\n",
    "    '4' : 'cancel',\n",
    "    '5' : 'service',\n",
    "    '6' : 'ticket',\n",
    "    '7' : 'corona'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        6300364\n",
       "4        177343\n",
       "2        134249\n",
       "5         89709\n",
       "1         62179\n",
       "6         53402\n",
       "7         17714\n",
       "24         8517\n",
       "45         5238\n",
       "46         4924\n",
       "12         4498\n",
       "25         3572\n",
       "15         2140\n",
       "16          966\n",
       "11          938\n",
       "56          922\n",
       "14          651\n",
       "26          635\n",
       "3           520\n",
       "47          511\n",
       "245         184\n",
       "125         173\n",
       "67          136\n",
       "456          83\n",
       "467          82\n",
       "124          79\n",
       "112          76\n",
       "145          31\n",
       "57           30\n",
       "115          29\n",
       "246          25\n",
       "27           13\n",
       "156          12\n",
       "126          11\n",
       "116          10\n",
       "256           7\n",
       "167           7\n",
       "114           6\n",
       "457           5\n",
       "146           5\n",
       "1245          4\n",
       "17            2\n",
       "1125          2\n",
       "257           1\n",
       "37            1\n",
       "567           1\n",
       "147           1\n",
       "Name: Topic, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result['Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5946556      1\n",
       "3811929      1\n",
       "3811907      1\n",
       "5868482      1\n",
       "4362259      1\n",
       "          ... \n",
       "7527681    NaN\n",
       "7527682    NaN\n",
       "7527683    NaN\n",
       "7527684    NaN\n",
       "7527685    NaN\n",
       "Name: Topic, Length: 6870008, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plot = df_result.replace('', float('NaN')).sort_values(by= 'Topic')\n",
    "df_plot['Topic']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export = df_result.drop('text', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wxport.to_csv('topic_id2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
